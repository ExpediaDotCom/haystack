health.status.path = "/app/isHealthy"

haystack.graphite.host = "monitoring-influxdb-graphite.kube-system.svc"

kafka {
  producer {
    topic = "proto-spans"
    props {
      bootstrap.servers = "kafkasvc:9092"
    }
  }
}

extractor {
  output.format = "proto"

  spans.validation {

    # Validate size of span. Truncate span tags when size exceeds spcified limit.
    # enable: true/false
    # log.only: if enabled, only logs such spans but doesn't truncate the tags
    # max.size.limit: maximum size allowed
    # message.tag.key: this tag key will be added when tags are truncated
    # message.tag.value: value of the above tag key indicating the truncation
    # skip.tags: truncate all span tags except these
    # skip.services: truncate span tags for all services except these
    max.size {
      enable = "false"
      log.only = "false"
      max.size.limit = 5000 // in bytes
      message.tag.key = "X-HAYSTACK-SPAN-INFO"
      message.tag.value = "Tags are truncated. REASON: Span Size Limit Exceeded. Please contact Haystack for more details"
      skip.tags = ["error"]
      skip.services = []
    }
  }
}

kinesis {
  #optional, uncomment following if you want to connect to kinesis using sts role arn
  #sts.role.arn = "provide the arn here"

  aws.region = "us-west-2"
  app.group.name = "haystack-kinesis-proto-span-collector"

  # optional, use endpoint property along with aws.region for cross region reading of data. Otherwise, you might see
  # latency issues for cross region access
  #endpoint = "vpce-xxxxxxxxx-yyyyyy.kinesis.us-east-1.vpce.amazonaws.com"

  stream {
    name = "haystack-proto-spans"
    position = "LATEST"
  }

  checkpoint {
    interval.ms = 15000
    retries = 50
    retry.interval.ms = 250
  }

  task.backoff.ms = 200
  max.records.read = 2000
  idle.time.between.reads.ms = 500
  shard.sync.interval.ms = 30000

  metrics {
    level = "NONE"
    buffer.time.ms = 15000
  }
}
